{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebf3793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "DATA_PATH = os.getenv('TEACHER_DIR', os.getcwd()) + '/JHL_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e66c4",
   "metadata": {},
   "source": [
    "### Let's try to create a differt type of model: A Convolutional Neural Network ('CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0cc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # conv: 1x28x28 -> 32x26x26\n",
    "        # relu\n",
    "        # conv: 32x26x26 -> 64x24x24\n",
    "        # relu\n",
    "        # max_pool: 64x24x24 -> 64x12x12\n",
    "        # flatten: 64x12x12 -> 9216\n",
    "        # linear: 9216 -> 128\n",
    "        # relu\n",
    "        # linear: 128 -> 10\n",
    "        # log_softmax\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "#         self.dropout1 = nn.Dropout(0.25)  # optional\n",
    "#         self.dropout2 = nn.Dropout(0.5)  # optional\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ade37a",
   "metadata": {},
   "source": [
    "### Insert your train loop that you created for the MLP here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88ead022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "            correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.2f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(),\n",
    "                correct/len(data)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186959d",
   "metadata": {},
   "source": [
    "### Insert your test loop that you created for the MLP here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a020486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100 * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5bf79",
   "metadata": {},
   "source": [
    "### Training hyperparameters and PyTorch boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a36b254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"CUDA is {'' if use_cuda else 'not '}available\")\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a90a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/robogast/Code/UvA_ML_2022/notebooks/JHL_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/robogast/Code/UvA_ML_2022/notebooks/JHL_data/MNIST/raw/train-images-idx3-ubyte.gz to /home/robogast/Code/UvA_ML_2022/notebooks/JHL_data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 2.192132\tAccuracy: 0.48\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 2.045283\tAccuracy: 0.60\n",
      "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 1.784880\tAccuracy: 0.66\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 1.371057\tAccuracy: 0.77\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m train_loader, test_loader \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     15\u001B[0m     torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(\n\u001B[1;32m     16\u001B[0m         datasets\u001B[38;5;241m.\u001B[39mMNIST(DATA_PATH, train\u001B[38;5;241m=\u001B[39mtrain, transform\u001B[38;5;241m=\u001B[39mtransform, download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     21\u001B[0m )\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(EPOCHS):\n\u001B[0;32m---> 24\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mLOG_INTERVAL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m     test(model, device, test_loader)\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, device, train_loader, optimizer, epoch, log_interval)\u001B[0m\n\u001B[1;32m      6\u001B[0m output \u001B[38;5;241m=\u001B[39m model(data)\n\u001B[1;32m      7\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mnll_loss(output, target)\n\u001B[0;32m----> 8\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_idx \u001B[38;5;241m%\u001B[39m log_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/tensor.py:245\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    237\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    238\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    239\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    243\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    244\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 245\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:145\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    143\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m--> 145\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 600\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.01\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Creates the PyTorch tensors from the PIL images, and normalizes them to the [0, 1] interval \n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalizes the data to 0 mean and 1 standard deviation\n",
    "])\n",
    "\n",
    "train_loader, test_loader = (\n",
    "    torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(DATA_PATH, train=train, transform=transform, download=True),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    for train in (True, False)\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(model, device, train_loader, optimizer, epoch, LOG_INTERVAL)\n",
    "    test(model, device, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a412bd",
   "metadata": {},
   "source": [
    "### Do you notice something when comparing the training and validation losses?\n",
    "### Is there something you can do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CIFAR10\n",
    "\n",
    "MNIST is the easiest widely-used dataset as a computer vision toy-problem.\n",
    "One step above MNIST is CIFAR10, which contains images from 10 classes of objects:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 4 convolution layers, each followed by _batch-normalization_\n",
    "        # (try to find out what this is and why!), and a non-linearity\n",
    "        # maxpooling after the activations of the 2nd, 3rd, and 4th conv layers\n",
    "        # 2 dense layers for classification\n",
    "        # log_softmax\n",
    "        #\n",
    "        # As for the number of channels of each layers, try to experiment!\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=10),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.349461\tAccuracy: 0.11\n",
      "Train Epoch: 0 [1280/50000 (3%)]\tLoss: 2.267515\tAccuracy: 0.25\n",
      "Train Epoch: 0 [2560/50000 (5%)]\tLoss: 2.083044\tAccuracy: 0.25\n",
      "Train Epoch: 0 [3840/50000 (8%)]\tLoss: 1.839039\tAccuracy: 0.30\n",
      "Train Epoch: 0 [5120/50000 (10%)]\tLoss: 1.778747\tAccuracy: 0.33\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 1.719884\tAccuracy: 0.38\n",
      "Train Epoch: 0 [7680/50000 (15%)]\tLoss: 1.539471\tAccuracy: 0.42\n",
      "Train Epoch: 0 [8960/50000 (18%)]\tLoss: 1.634732\tAccuracy: 0.41\n",
      "Train Epoch: 0 [10240/50000 (20%)]\tLoss: 1.593994\tAccuracy: 0.38\n",
      "Train Epoch: 0 [11520/50000 (23%)]\tLoss: 1.541728\tAccuracy: 0.45\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.449866\tAccuracy: 0.45\n",
      "Train Epoch: 0 [14080/50000 (28%)]\tLoss: 1.405400\tAccuracy: 0.51\n",
      "Train Epoch: 0 [15360/50000 (31%)]\tLoss: 1.334575\tAccuracy: 0.52\n",
      "Train Epoch: 0 [16640/50000 (33%)]\tLoss: 1.465970\tAccuracy: 0.49\n",
      "Train Epoch: 0 [17920/50000 (36%)]\tLoss: 1.431089\tAccuracy: 0.46\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.239824\tAccuracy: 0.57\n",
      "Train Epoch: 0 [20480/50000 (41%)]\tLoss: 1.300315\tAccuracy: 0.51\n",
      "Train Epoch: 0 [21760/50000 (43%)]\tLoss: 1.241092\tAccuracy: 0.53\n",
      "Train Epoch: 0 [23040/50000 (46%)]\tLoss: 1.138416\tAccuracy: 0.59\n",
      "Train Epoch: 0 [24320/50000 (49%)]\tLoss: 0.949394\tAccuracy: 0.69\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.132093\tAccuracy: 0.57\n",
      "Train Epoch: 0 [26880/50000 (54%)]\tLoss: 1.132695\tAccuracy: 0.57\n",
      "Train Epoch: 0 [28160/50000 (56%)]\tLoss: 1.191099\tAccuracy: 0.53\n",
      "Train Epoch: 0 [29440/50000 (59%)]\tLoss: 1.076093\tAccuracy: 0.62\n",
      "Train Epoch: 0 [30720/50000 (61%)]\tLoss: 1.160238\tAccuracy: 0.61\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.141936\tAccuracy: 0.59\n",
      "Train Epoch: 0 [33280/50000 (66%)]\tLoss: 1.011961\tAccuracy: 0.66\n",
      "Train Epoch: 0 [34560/50000 (69%)]\tLoss: 1.096020\tAccuracy: 0.60\n",
      "Train Epoch: 0 [35840/50000 (72%)]\tLoss: 1.152684\tAccuracy: 0.62\n",
      "Train Epoch: 0 [37120/50000 (74%)]\tLoss: 1.103749\tAccuracy: 0.62\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.059401\tAccuracy: 0.57\n",
      "Train Epoch: 0 [39680/50000 (79%)]\tLoss: 1.023019\tAccuracy: 0.66\n",
      "Train Epoch: 0 [40960/50000 (82%)]\tLoss: 1.010137\tAccuracy: 0.62\n",
      "Train Epoch: 0 [42240/50000 (84%)]\tLoss: 0.997688\tAccuracy: 0.66\n",
      "Train Epoch: 0 [43520/50000 (87%)]\tLoss: 1.017875\tAccuracy: 0.61\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.058273\tAccuracy: 0.60\n",
      "Train Epoch: 0 [46080/50000 (92%)]\tLoss: 0.917330\tAccuracy: 0.66\n",
      "Train Epoch: 0 [47360/50000 (95%)]\tLoss: 1.107717\tAccuracy: 0.59\n",
      "Train Epoch: 0 [48640/50000 (97%)]\tLoss: 1.025527\tAccuracy: 0.58\n",
      "Train Epoch: 0 [31200/50000 (100%)]\tLoss: 0.874011\tAccuracy: 0.72\n",
      "\n",
      "Test set: Average loss: 1.0186, Accuracy: 6445/10000 (64%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.919069\tAccuracy: 0.69\n",
      "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 0.877630\tAccuracy: 0.67\n",
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 0.944297\tAccuracy: 0.64\n",
      "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 0.856215\tAccuracy: 0.66\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 0.726478\tAccuracy: 0.70\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.138123\tAccuracy: 0.65\n",
      "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 0.754450\tAccuracy: 0.76\n",
      "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 0.824393\tAccuracy: 0.69\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 0.991005\tAccuracy: 0.65\n",
      "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 0.872382\tAccuracy: 0.69\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.820826\tAccuracy: 0.73\n",
      "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 0.904879\tAccuracy: 0.66\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 0.690079\tAccuracy: 0.76\n",
      "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 0.907371\tAccuracy: 0.70\n",
      "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 0.791832\tAccuracy: 0.73\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.059204\tAccuracy: 0.61\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 0.814595\tAccuracy: 0.73\n",
      "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 0.808979\tAccuracy: 0.72\n",
      "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 0.899847\tAccuracy: 0.69\n",
      "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 0.809738\tAccuracy: 0.73\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.714818\tAccuracy: 0.78\n",
      "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 0.718748\tAccuracy: 0.73\n",
      "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 0.967366\tAccuracy: 0.66\n",
      "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 0.738367\tAccuracy: 0.76\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 0.793177\tAccuracy: 0.70\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.762247\tAccuracy: 0.70\n",
      "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 1.017744\tAccuracy: 0.70\n",
      "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 0.871064\tAccuracy: 0.69\n",
      "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 0.883533\tAccuracy: 0.71\n",
      "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 0.752852\tAccuracy: 0.74\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.720210\tAccuracy: 0.75\n",
      "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 0.631003\tAccuracy: 0.75\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 0.646163\tAccuracy: 0.77\n",
      "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 0.874492\tAccuracy: 0.66\n",
      "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 0.804095\tAccuracy: 0.75\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.842371\tAccuracy: 0.75\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 0.770194\tAccuracy: 0.75\n",
      "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 0.923747\tAccuracy: 0.65\n",
      "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 0.924649\tAccuracy: 0.65\n",
      "Train Epoch: 1 [31200/50000 (100%)]\tLoss: 0.782547\tAccuracy: 0.68\n",
      "\n",
      "Test set: Average loss: 0.9814, Accuracy: 6639/10000 (66%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.693610\tAccuracy: 0.70\n",
      "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 0.680984\tAccuracy: 0.73\n",
      "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 0.673898\tAccuracy: 0.77\n",
      "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 0.771810\tAccuracy: 0.73\n",
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 0.614455\tAccuracy: 0.78\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.760324\tAccuracy: 0.73\n",
      "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 0.633787\tAccuracy: 0.77\n",
      "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 0.881170\tAccuracy: 0.73\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 0.846467\tAccuracy: 0.64\n",
      "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 0.646566\tAccuracy: 0.80\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.712317\tAccuracy: 0.75\n",
      "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 0.733794\tAccuracy: 0.74\n",
      "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 0.586181\tAccuracy: 0.77\n",
      "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 0.715847\tAccuracy: 0.74\n",
      "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 0.607596\tAccuracy: 0.81\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.636852\tAccuracy: 0.76\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 0.728211\tAccuracy: 0.76\n",
      "Train Epoch: 2 [21760/50000 (43%)]\tLoss: 0.923603\tAccuracy: 0.64\n",
      "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 0.545625\tAccuracy: 0.80\n",
      "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 0.614582\tAccuracy: 0.76\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.634627\tAccuracy: 0.78\n",
      "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 0.648896\tAccuracy: 0.76\n",
      "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 0.658826\tAccuracy: 0.71\n",
      "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 0.953371\tAccuracy: 0.68\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 0.721372\tAccuracy: 0.77\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.781892\tAccuracy: 0.70\n",
      "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 0.522332\tAccuracy: 0.84\n",
      "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 0.707494\tAccuracy: 0.73\n",
      "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 0.669462\tAccuracy: 0.74\n",
      "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 0.617652\tAccuracy: 0.77\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.616306\tAccuracy: 0.75\n",
      "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 0.676681\tAccuracy: 0.77\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 0.594405\tAccuracy: 0.80\n",
      "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 0.839948\tAccuracy: 0.66\n",
      "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 0.644296\tAccuracy: 0.77\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.682127\tAccuracy: 0.76\n",
      "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 0.576321\tAccuracy: 0.82\n",
      "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 0.479427\tAccuracy: 0.84\n",
      "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 0.698294\tAccuracy: 0.73\n",
      "Train Epoch: 2 [31200/50000 (100%)]\tLoss: 0.539367\tAccuracy: 0.85\n",
      "\n",
      "Test set: Average loss: 0.7050, Accuracy: 7498/10000 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.422339\tAccuracy: 0.87\n",
      "Train Epoch: 3 [1280/50000 (3%)]\tLoss: 0.459863\tAccuracy: 0.80\n",
      "Train Epoch: 3 [2560/50000 (5%)]\tLoss: 0.529256\tAccuracy: 0.77\n",
      "Train Epoch: 3 [3840/50000 (8%)]\tLoss: 0.743565\tAccuracy: 0.73\n",
      "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 0.545448\tAccuracy: 0.85\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.489730\tAccuracy: 0.83\n",
      "Train Epoch: 3 [7680/50000 (15%)]\tLoss: 0.503850\tAccuracy: 0.79\n",
      "Train Epoch: 3 [8960/50000 (18%)]\tLoss: 0.617128\tAccuracy: 0.77\n",
      "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 0.674523\tAccuracy: 0.76\n",
      "Train Epoch: 3 [11520/50000 (23%)]\tLoss: 0.632496\tAccuracy: 0.76\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m train_loader, test_loader \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     20\u001B[0m     torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(\n\u001B[1;32m     21\u001B[0m         datasets\u001B[38;5;241m.\u001B[39mCIFAR10(DATA_PATH, train\u001B[38;5;241m=\u001B[39mtrain, transform\u001B[38;5;241m=\u001B[39mtransform, download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     27\u001B[0m )\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(EPOCHS):\n\u001B[0;32m---> 30\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mLOG_INTERVAL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     test(model, device, test_loader)\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, device, train_loader, optimizer, epoch, log_interval)\u001B[0m\n\u001B[1;32m      7\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mnll_loss(output, target)\n\u001B[1;32m      8\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m----> 9\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_idx \u001B[38;5;241m%\u001B[39m log_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     11\u001B[0m     pred \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:89\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     87\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m---> 89\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adam.py:108\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    105\u001B[0m             state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    107\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m--> 108\u001B[0m     \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m           \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[43m           \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[43m           \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[43m           \u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[43m           \u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m           \u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m           \u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/optim/_functional.py:85\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[1;32m     84\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mmul_(beta1)\u001B[38;5;241m.\u001B[39madd_(grad, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[0;32m---> 85\u001B[0m \u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amsgrad:\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001B[39;00m\n\u001B[1;32m     88\u001B[0m     torch\u001B[38;5;241m.\u001B[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001B[38;5;241m=\u001B[39mmax_exp_avg_sqs[i])\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# We can Re-use our train and test loops\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "model = CIFAR10CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) # optim.<OPTIMIZER_FLAVOUR>(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the data to 0 mean and 1 standard deviation, now for all channels of RGB\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "\n",
    "train_loader, test_loader = (\n",
    "    torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(DATA_PATH, train=train, transform=transform, download=True),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "    for train in (True, False)\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(model, device, train_loader, optimizer, epoch, LOG_INTERVAL)\n",
    "    test(model, device, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}